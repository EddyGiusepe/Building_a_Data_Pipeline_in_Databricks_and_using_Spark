{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Data Pipeline in Databricks and using Spark\n",
    "---\n",
    "\n",
    "#### Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Configura√ß√£o do Databricks Connect\n",
    "\n",
    "Este notebook est√° configurado para usar **Databricks Connect**, permitindo executar c√≥digo localmente mas processando dados no cluster Databricks.\n",
    "\n",
    "## üîê Informa√ß√µes necess√°rias:\n",
    "\n",
    "1. **Server hostname**: URL do seu workspace Databricks\n",
    "2. **Personal Access Token**: Token de acesso (User Settings ‚Üí Developer ‚Üí Access Tokens)\n",
    "3. **Cluster ID**: ID do cluster ativo (copie da URL quando abrir um cluster)\n",
    "\n",
    "## üìã Como configurar:\n",
    "\n",
    "### 1Ô∏è‚É£ Criar vari√°veis de ambiente (recomendado):\n",
    "```bash\n",
    "export DATABRICKS_HOST=\"https://seu-workspace.cloud.databricks.com\"\n",
    "export DATABRICKS_TOKEN=\"dapi-seu-token-aqui\"  \n",
    "export DATABRICKS_CLUSTER_ID=\"cluster-id-aqui\"\n",
    "```\n",
    "\n",
    "### 2Ô∏è‚É£ Ou configure diretamente no c√≥digo abaixo ‚¨áÔ∏è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìã COMO OBTER AS CREDENCIAIS DO DATABRICKS\n",
    "\n",
    "## üîç 1. Server Hostname\n",
    "- Va para seu workspace Databricks\n",
    "- Copie a URL do navegador (ex: `https://dbc-a1b2c3d4-e5f6.cloud.databricks.com`)\n",
    "\n",
    "## üîë 2. Personal Access Token\n",
    "1. No Databricks workspace ‚Üí clique no seu **avatar** (canto superior direito)\n",
    "2. **User Settings**\n",
    "3. **Developer** (no menu lateral)\n",
    "4. **Access Tokens**\n",
    "5. **Generate New Token**\n",
    "6. D√™ um nome (ex: \"Local Development\")\n",
    "7. Defina expira√ß√£o (recomendo 90 dias)\n",
    "8. **Generate** ‚Üí copie o token (come√ßa com `dapi-...`)\n",
    "\n",
    "## üíª 3. Cluster ID\n",
    "1. No Databricks workspace ‚Üí **Compute** (menu lateral)\n",
    "2. Clique no cluster que deseja usar\n",
    "3. Copie o **Cluster ID** da URL ou das configura√ß√µes do cluster\n",
    "\n",
    "## üîí 4. Configurar de forma segura\n",
    "### Op√ß√£o A: Vari√°veis de ambiente (recomendado)\n",
    "```bash\n",
    "export DATABRICKS_HOST=\"https://seu-workspace.cloud.databricks.com\"\n",
    "export DATABRICKS_TOKEN=\"dapi-seu-token-aqui\"\n",
    "export DATABRICKS_CLUSTER_ID=\"cluster-id-aqui\"\n",
    "```\n",
    "\n",
    "### Op√ß√£o B: Arquivo .env (alternativa)\n",
    "Crie um arquivo `.env` na pasta do projeto:\n",
    "```\n",
    "DATABRICKS_HOST=https://seu-workspace.cloud.databricks.com\n",
    "DATABRICKS_TOKEN=dapi-seu-token-aqui\n",
    "DATABRICKS_CLUSTER_ID=cluster-id-aqui\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE**: Nunca fa√ßa commit de tokens/credenciais para Git!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Conectando ao Databricks...\n",
      "üìç Host: https://dbc-15e1a878-daba.cloud.databricks.com\n",
      "üÜî Cluster: 0919-175003-lz63w6ap-v2n\n",
      "‚úÖ Conectado ao Databricks com sucesso!\n",
      "üîß Vers√£o Spark: 4.0.0\n",
      "\n",
      "üìÇ Carregando dados do Volume: /Volumes/workspace/default_eddy/volumeeddy-tmp-sampledata/sample_data.csv\n",
      "üéâ DADOS DO VOLUME CARREGADOS COM SUCESSO!\n",
      "\n",
      "üìä Primeiros registros:\n",
      "+-------+---+-----------+\n",
      "|   Name|Age|       City|\n",
      "+-------+---+-----------+\n",
      "|  Alice| 25|   New York|\n",
      "|    Bob| 17|Los Angeles|\n",
      "|Charlie| 35|    Chicago|\n",
      "|  Diana| 16|    Houston|\n",
      "| Edward| 45|    Phoenix|\n",
      "+-------+---+-----------+\n",
      "\n",
      "\n",
      "üìà Informa√ß√µes do Dataset:\n",
      "   ‚Ä¢ Linhas: 5\n",
      "   ‚Ä¢ Colunas: 3\n",
      "   ‚Ä¢ Nomes das Colunas: ['Name', 'Age', 'City']\n",
      "\n",
      "üìã Schema do Dataset:\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n",
      "\n",
      "‚úÖ Dados tamb√©m dispon√≠veis como Pandas DataFrame para an√°lises locais!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Carregar credenciais\n",
    "_ = load_dotenv(find_dotenv())\n",
    "databricks_host = os.environ['DATABRICKS_HOST']\n",
    "databricks_token = os.environ['DATABRICKS_TOKEN']\n",
    "databricks_cluster_id = os.environ['DATABRICKS_CLUSTER_ID']\n",
    "\n",
    "print(\"üîó Conectando ao Databricks...\")\n",
    "print(f\"üìç Host: {databricks_host}\")\n",
    "print(f\"üÜî Cluster: {databricks_cluster_id}\")\n",
    "\n",
    "# Conectar ao Databricks\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "spark = DatabricksSession.builder \\\n",
    "    .remote(\n",
    "        host=databricks_host,\n",
    "        token=databricks_token,\n",
    "        cluster_id=databricks_cluster_id\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Conectado ao Databricks com sucesso!\")\n",
    "print(f\"üîß Vers√£o Spark: {spark.version}\")\n",
    "\n",
    "# üìÅ CARREGAR DADOS DO VOLUME DATABRICKS\n",
    "volume_path = \"/Volumes/workspace/default_eddy/volumeeddy-tmp-sampledata/sample_data.csv\"\n",
    "print(f\"\\nüìÇ Carregando dados do Volume: {volume_path}\")\n",
    "\n",
    "df_spark = spark.read.csv(volume_path, header=True, inferSchema=True)\n",
    "\n",
    "print(\"üéâ DADOS DO VOLUME CARREGADOS COM SUCESSO!\")\n",
    "print(\"\\nüìä Primeiros registros:\")\n",
    "df_spark.show()\n",
    "\n",
    "print(f\"\\nüìà Informa√ß√µes do Dataset:\")\n",
    "print(f\"   ‚Ä¢ Linhas: {df_spark.count()}\")\n",
    "print(f\"   ‚Ä¢ Colunas: {len(df_spark.columns)}\")\n",
    "print(f\"   ‚Ä¢ Nomes das Colunas: {df_spark.columns}\")\n",
    "\n",
    "print(\"\\nüìã Schema do Dataset:\")\n",
    "df_spark.printSchema()\n",
    "\n",
    "# Converter para Pandas para an√°lises adicionais (opcional)\n",
    "df_pandas = df_spark.toPandas()\n",
    "print(f\"\\n‚úÖ Dados tamb√©m dispon√≠veis como Pandas DataFrame para an√°lises locais!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä AN√ÅLISE COMPLETA DOS DADOS DO VOLUME DATABRICKS\n",
      "======================================================================\n",
      "\n",
      "üìä Estat√≠sticas Descritivas (processadas no cluster Databricks):\n",
      "+-------+------+------------------+-------+\n",
      "|summary|  Name|               Age|   City|\n",
      "+-------+------+------------------+-------+\n",
      "|  count|     5|                 5|      5|\n",
      "|   mean|  NULL|              27.6|   NULL|\n",
      "| stddev|  NULL|12.361229712289955|   NULL|\n",
      "|    min| Alice|                16|Chicago|\n",
      "|    max|Edward|                45|Phoenix|\n",
      "+-------+------+------------------+-------+\n",
      "\n",
      "\n",
      "üîç An√°lises com Spark SQL:\n",
      "\n",
      "üë• An√°lise de Idade:\n",
      "+-----------+------------+------------+-------------+\n",
      "|idade_media|idade_minima|idade_maxima|total_pessoas|\n",
      "+-----------+------------+------------+-------------+\n",
      "|       27.6|          16|          45|            5|\n",
      "+-----------+------------+------------+-------------+\n",
      "\n",
      "\n",
      "üîû Pessoas maiores de idade:\n",
      "+-------+---+--------+\n",
      "|   Name|Age|    City|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|New York|\n",
      "|Charlie| 35| Chicago|\n",
      "| Edward| 45| Phoenix|\n",
      "+-------+---+--------+\n",
      "\n",
      "üìà 3 de 5 pessoas s√£o maiores de idade (60.0%)\n",
      "\n",
      "üèôÔ∏è Contagem por Cidade:\n",
      "+-----------+----------+\n",
      "|       City|quantidade|\n",
      "+-----------+----------+\n",
      "|Los Angeles|         1|\n",
      "|   New York|         1|\n",
      "|    Houston|         1|\n",
      "|    Phoenix|         1|\n",
      "|    Chicago|         1|\n",
      "+-----------+----------+\n",
      "\n",
      "\n",
      "üìà An√°lises Avan√ßadas:\n",
      "+-------+---+-----------+---------------+--------------------+\n",
      "|   Name|Age|       City|categoria_idade|ranking_idade_cidade|\n",
      "+-------+---+-----------+---------------+--------------------+\n",
      "|Charlie| 35|    Chicago|         Adulto|                   1|\n",
      "|  Diana| 16|    Houston| Menor de idade|                   1|\n",
      "|    Bob| 17|Los Angeles| Menor de idade|                   1|\n",
      "|  Alice| 25|   New York|         Adulto|                   1|\n",
      "| Edward| 45|    Phoenix|         Adulto|                   1|\n",
      "+-------+---+-----------+---------------+--------------------+\n",
      "\n",
      "\n",
      "üêº An√°lises complementares com Pandas:\n",
      "   ‚Ä¢ Dataset convertido: 5 linhas, 3 colunas\n",
      "   ‚Ä¢ Idade m√©dia: 27.60 anos\n",
      "   ‚Ä¢ Cidades √∫nicas: 5\n",
      "\n",
      "‚úÖ Pipeline ETL com Databricks Volume conclu√≠do com sucesso!\n",
      "   ‚Ä¢ üå©Ô∏è Processamento: Cluster Databricks\n",
      "   ‚Ä¢ üìÅ Fonte: Volume Databricks\n",
      "   ‚Ä¢ üìä Registros: 5\n",
      "   ‚Ä¢ üöÄ Engine: Spark 4.0.0\n",
      "   ‚Ä¢ üíæ Dispon√≠vel em: Spark DataFrame + Pandas DataFrame\n"
     ]
    }
   ],
   "source": [
    "# üìä AN√ÅLISE AVAN√áADA DOS DADOS DO VOLUME DATABRICKS\n",
    "print(\"=\"*70)\n",
    "print(\"üìä AN√ÅLISE COMPLETA DOS DADOS DO VOLUME DATABRICKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Estat√≠sticas descritivas com Spark\n",
    "print(\"\\nüìä Estat√≠sticas Descritivas (processadas no cluster Databricks):\")\n",
    "df_spark.describe().show()\n",
    "\n",
    "# An√°lises com Spark SQL\n",
    "print(\"\\nüîç An√°lises com Spark SQL:\")\n",
    "\n",
    "# Registrar como view tempor√°ria para usar SQL\n",
    "df_spark.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "# 1. An√°lise de idade\n",
    "print(\"\\nüë• An√°lise de Idade:\")\n",
    "idade_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        AVG(Age) as idade_media,\n",
    "        MIN(Age) as idade_minima,\n",
    "        MAX(Age) as idade_maxima,\n",
    "        COUNT(*) as total_pessoas\n",
    "    FROM pessoas\n",
    "\"\"\")\n",
    "idade_stats.show()\n",
    "\n",
    "# 2. Maiores de idade\n",
    "print(\"\\nüîû Pessoas maiores de idade:\")\n",
    "adults = spark.sql(\"SELECT * FROM pessoas WHERE Age >= 18\")\n",
    "adults.show()\n",
    "\n",
    "adult_count = adults.count()\n",
    "total_count = df_spark.count()\n",
    "print(f\"üìà {adult_count} de {total_count} pessoas s√£o maiores de idade ({adult_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# 3. An√°lise por cidade\n",
    "print(\"\\nüèôÔ∏è Contagem por Cidade:\")\n",
    "city_analysis = spark.sql(\"\"\"\n",
    "    SELECT City, COUNT(*) as quantidade\n",
    "    FROM pessoas \n",
    "    GROUP BY City \n",
    "    ORDER BY quantidade DESC\n",
    "\"\"\")\n",
    "city_analysis.show()\n",
    "\n",
    "# 4. An√°lises avan√ßadas com Window Functions\n",
    "print(\"\\nüìà An√°lises Avan√ßadas:\")\n",
    "advanced_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Name,\n",
    "        Age,\n",
    "        City,\n",
    "        CASE \n",
    "            WHEN Age >= 18 THEN 'Adulto'\n",
    "            ELSE 'Menor de idade'\n",
    "        END as categoria_idade,\n",
    "        ROW_NUMBER() OVER (PARTITION BY City ORDER BY Age DESC) as ranking_idade_cidade\n",
    "    FROM pessoas\n",
    "    ORDER BY City, Age DESC\n",
    "\"\"\")\n",
    "advanced_analysis.show()\n",
    "\n",
    "# Converter para Pandas para an√°lises complementares\n",
    "print(\"\\nüêº An√°lises complementares com Pandas:\")\n",
    "df_pandas = df_spark.toPandas()\n",
    "print(f\"   ‚Ä¢ Dataset convertido: {df_pandas.shape[0]} linhas, {df_pandas.shape[1]} colunas\")\n",
    "print(f\"   ‚Ä¢ Idade m√©dia: {df_pandas['Age'].mean():.2f} anos\")\n",
    "print(f\"   ‚Ä¢ Cidades √∫nicas: {df_pandas['City'].nunique()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline ETL com Databricks Volume conclu√≠do com sucesso!\")\n",
    "print(\"   ‚Ä¢ üå©Ô∏è Processamento: Cluster Databricks\")\n",
    "print(\"   ‚Ä¢ üìÅ Fonte: Volume Databricks\")\n",
    "print(f\"   ‚Ä¢ üìä Registros: {df_spark.count()}\")\n",
    "print(f\"   ‚Ä¢ üöÄ Engine: Spark {spark.version}\")\n",
    "print(\"   ‚Ä¢ üíæ Dispon√≠vel em: Spark DataFrame + Pandas DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1728710-0082-4be5-a91d-b8cb48508b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline_ETL",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
