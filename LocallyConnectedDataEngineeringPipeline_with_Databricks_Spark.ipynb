{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\"><font color=\"gree\">Building a Data Pipeline in Databricks and using Spark</font></h1>\n",
    "---\n",
    "\n",
    "<font color=\"pink\">Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link de estudo:\n",
    "\n",
    "* [Databricks Caderno de anota√ß√µes](https://docs.databricks.com/aws/pt/notebooks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">üöÄ Configura√ß√£o do Databricks Connect</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook est√° configurado para usar **Databricks Connect**, permitindo executar c√≥digo localmente mas processando dados no `cluster Databricks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">üîê Informa√ß√µes necess√°rias</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Server hostname`: URL do seu workspace Databricks\n",
    "2. `Personal Access Token`: Token de acesso (User Settings ‚Üí Developer ‚Üí Access Tokens)\n",
    "3. `Cluster ID`: ID do cluster ativo (copie da URL quando abrir um cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Como configurar?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Op√ß√£o A`: Voc√™ pode criar vari√°veis de ambiente:\n",
    "\n",
    "```bash\n",
    "export DATABRICKS_HOST=\"https://seu-workspace.cloud.databricks.com\"\n",
    "export DATABRICKS_TOKEN=\"dapi-seu-token-aqui\"  \n",
    "export DATABRICKS_CLUSTER_ID=\"cluster-id-aqui\"\n",
    "```\n",
    "\n",
    "* `Op√ß√£o B`: Crie um arquivo `.env` na pasta do projeto:\n",
    "\n",
    "```bash\n",
    "DATABRICKS_HOST=https://seu-workspace.cloud.databricks.com\n",
    "DATABRICKS_TOKEN=dapi-seu-token-aqui\n",
    "DATABRICKS_CLUSTER_ID=cluster-id-aqui\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE**: Nunca fa√ßa commit de tokens/credenciais para Git!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">üìã COMO OBTER AS CREDENCIAIS DO DATABRICKS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">üîç 1. Server Hostname</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Va para seu workspace Databricks\n",
    "\n",
    "- Copie a URL do navegador (ex: `https://dbc-a1b2c3d4-e5f6.cloud.databricks.com`)\n",
    "\n",
    "\n",
    "Voc√™ pode copiar a partir do browser s√≥ ate `...com`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">üîë 2. Personal Access Token</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. No Databricks workspace ‚Üí clique no seu **avatar** (canto superior direito)\n",
    "\n",
    "2. **User Settings**\n",
    "\n",
    "3. **Developer** (no menu lateral)\n",
    "\n",
    "4. **Access Tokens**\n",
    "\n",
    "5. **Generate New Token**\n",
    "\n",
    "6. D√™ um nome (ex: \"Local Development\")\n",
    "\n",
    "7. Defina expira√ß√£o (recomendo 90 dias)\n",
    "\n",
    "8. **Generate** ‚Üí copie o token (come√ßa com `dapi-...`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">üíª 3. Cluster ID</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. No Databricks workspace ‚Üí **Compute** (menu lateral)\n",
    "\n",
    "2. Clique no cluster que deseja usar\n",
    "\n",
    "3. Copie o **Cluster ID** da URL ou das configura√ß√µes do cluster\n",
    "\n",
    "\n",
    "`NOTA`:\n",
    "\n",
    "Eu s√≥ consegui obter o ID do meu Cluster executando o seguinte comando na pr√≥pria c√©lula do Databricks:\n",
    "\n",
    "```python\n",
    "databricks_cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "\n",
    "print(f\"Databricks Cluster ID: {databricks_cluster_id}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">ETL Pipeline com dados armazenados no meu Volume Databricks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Carregado minhas credenciais do Databricks, com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "databricks_host = os.environ['DATABRICKS_HOST']\n",
    "databricks_token = os.environ['DATABRICKS_TOKEN']\n",
    "databricks_cluster_id = os.environ['DATABRICKS_CLUSTER_ID']\n",
    "\n",
    "print(\"üîó Carregado minhas credenciais do Databricks, com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir vamos usar `Databricks Connect` para conectar ao Databricks e executar c√≥digo `Spark` localmente, mas processando os dados remotamente no cluster Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado ao Databricks com sucesso!\n",
      "üîß Vers√£o Spark: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "from databricks.connect import DatabricksSession # Esta classe √© usada para conectar ao Databricks remotamente\n",
    "\n",
    "spark = DatabricksSession.builder.remote(host=databricks_host, token=databricks_token, cluster_id=databricks_cluster_id).getOrCreate()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Conectado ao Databricks com sucesso!\")\n",
    "print(f\"üîß Vers√£o Spark: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir vamos carregar dados do `Volume Databricks`. Basicamente, vou carregar esses dados a partir do `Catalog` do meu workspace Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_path = \"/Volumes/workspace/default_eddy/volumeeddy-tmp-sampledata/sample_data.csv\"\n",
    "print(f\"\\nüìÇ Carregando dados do Volume: {volume_path}\")\n",
    "\n",
    "df_spark = spark.read.csv(volume_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"üéâ DADOS DO VOLUME CARREGADOS COM SUCESSO!\")\n",
    "print(\"\\nüìä Primeiros registros:\")\n",
    "df_spark.show()\n",
    "\n",
    "print(f\"\\nüìà Informa√ß√µes do Dataset:\")\n",
    "print(f\"   ‚Ä¢ Linhas: {df_spark.count()}\")\n",
    "print(f\"   ‚Ä¢ Colunas: {len(df_spark.columns)}\")\n",
    "print(f\"   ‚Ä¢ Nomes das Colunas: {df_spark.columns}\")\n",
    "\n",
    "print(\"\\nüìã Schema do Dataset:\")\n",
    "df_spark.printSchema()\n",
    "\n",
    "# Converter para Pandas para an√°lises adicionais (opcional)\n",
    "df_pandas = df_spark.toPandas()\n",
    "print(f\"\\n‚úÖ Dados tamb√©m dispon√≠veis como Pandas DataFrame para an√°lises locais!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä AN√ÅLISE AVAN√áADA DOS DADOS DO VOLUME DATABRICKS\n",
    "print(\"=\"*70)\n",
    "print(\"üìä AN√ÅLISE COMPLETA DOS DADOS DO VOLUME DATABRICKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Estat√≠sticas descritivas com Spark\n",
    "print(\"\\nüìä Estat√≠sticas Descritivas (processadas no cluster Databricks):\")\n",
    "df_spark.describe().show()\n",
    "\n",
    "# An√°lises com Spark SQL\n",
    "print(\"\\nüîç An√°lises com Spark SQL:\")\n",
    "\n",
    "# Registrar como view tempor√°ria para usar SQL\n",
    "df_spark.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "# 1. An√°lise de idade\n",
    "print(\"\\nüë• An√°lise de Idade:\")\n",
    "idade_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        AVG(Age) as idade_media,\n",
    "        MIN(Age) as idade_minima,\n",
    "        MAX(Age) as idade_maxima,\n",
    "        COUNT(*) as total_pessoas\n",
    "    FROM pessoas\n",
    "\"\"\")\n",
    "idade_stats.show()\n",
    "\n",
    "# 2. Maiores de idade\n",
    "print(\"\\nüîû Pessoas maiores de idade:\")\n",
    "adults = spark.sql(\"SELECT * FROM pessoas WHERE Age >= 18\")\n",
    "adults.show()\n",
    "\n",
    "adult_count = adults.count()\n",
    "total_count = df_spark.count()\n",
    "print(f\"üìà {adult_count} de {total_count} pessoas s√£o maiores de idade ({adult_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# 3. An√°lise por cidade\n",
    "print(\"\\nüèôÔ∏è Contagem por Cidade:\")\n",
    "city_analysis = spark.sql(\"\"\"\n",
    "    SELECT City, COUNT(*) as quantidade\n",
    "    FROM pessoas \n",
    "    GROUP BY City \n",
    "    ORDER BY quantidade DESC\n",
    "\"\"\")\n",
    "city_analysis.show()\n",
    "\n",
    "# 4. An√°lises avan√ßadas com Window Functions\n",
    "print(\"\\nüìà An√°lises Avan√ßadas:\")\n",
    "advanced_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Name,\n",
    "        Age,\n",
    "        City,\n",
    "        CASE \n",
    "            WHEN Age >= 18 THEN 'Adulto'\n",
    "            ELSE 'Menor de idade'\n",
    "        END as categoria_idade,\n",
    "        ROW_NUMBER() OVER (PARTITION BY City ORDER BY Age DESC) as ranking_idade_cidade\n",
    "    FROM pessoas\n",
    "    ORDER BY City, Age DESC\n",
    "\"\"\")\n",
    "advanced_analysis.show()\n",
    "\n",
    "# Converter para Pandas para an√°lises complementares\n",
    "print(\"\\nüêº An√°lises complementares com Pandas:\")\n",
    "df_pandas = df_spark.toPandas()\n",
    "print(f\"   ‚Ä¢ Dataset convertido: {df_pandas.shape[0]} linhas, {df_pandas.shape[1]} colunas\")\n",
    "print(f\"   ‚Ä¢ Idade m√©dia: {df_pandas['Age'].mean():.2f} anos\")\n",
    "print(f\"   ‚Ä¢ Cidades √∫nicas: {df_pandas['City'].nunique()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline ETL com Databricks Volume conclu√≠do com sucesso!\")\n",
    "print(\"   ‚Ä¢ üå©Ô∏è Processamento: Cluster Databricks\")\n",
    "print(\"   ‚Ä¢ üìÅ Fonte: Volume Databricks\")\n",
    "print(f\"   ‚Ä¢ üìä Registros: {df_spark.count()}\")\n",
    "print(f\"   ‚Ä¢ üöÄ Engine: Spark {spark.version}\")\n",
    "print(\"   ‚Ä¢ üíæ Dispon√≠vel em: Spark DataFrame + Pandas DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline_ETL",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
