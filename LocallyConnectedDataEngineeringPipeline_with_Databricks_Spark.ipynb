{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\"><font color=\"gree\">Building a Data Pipeline in Databricks and using Spark</font></h1>\n",
    "---\n",
    "\n",
    "<font color=\"pink\">Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link de estudo:\n",
    "\n",
    "* [Databricks Caderno de anota√ß√µes](https://docs.databricks.com/aws/pt/notebooks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">üöÄ Configura√ß√£o do Databricks Connect</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook est√° configurado para usar **Databricks Connect**, permitindo executar c√≥digo localmente mas processando dados no `cluster Databricks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">üîê Informa√ß√µes necess√°rias</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Server hostname`: URL do seu workspace Databricks\n",
    "2. `Personal Access Token`: Token de acesso (User Settings ‚Üí Developer ‚Üí Access Tokens)\n",
    "3. `Cluster ID`: ID do cluster ativo (copie da URL quando abrir um cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Como configurar?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Op√ß√£o A`: Voc√™ pode criar vari√°veis de ambiente:\n",
    "\n",
    "```bash\n",
    "export DATABRICKS_HOST=\"https://seu-workspace.cloud.databricks.com\"\n",
    "export DATABRICKS_TOKEN=\"dapi-seu-token-aqui\"  \n",
    "export DATABRICKS_CLUSTER_ID=\"cluster-id-aqui\"\n",
    "```\n",
    "\n",
    "* `Op√ß√£o B`: Crie um arquivo `.env` na pasta do projeto:\n",
    "\n",
    "```bash\n",
    "DATABRICKS_HOST=https://seu-workspace.cloud.databricks.com\n",
    "DATABRICKS_TOKEN=dapi-seu-token-aqui\n",
    "DATABRICKS_CLUSTER_ID=cluster-id-aqui\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE**: Nunca fa√ßa commit de tokens/credenciais para Git!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">üìã COMO OBTER AS CREDENCIAIS DO DATABRICKS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">üîç 1. Server Hostname</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Va para seu workspace Databricks\n",
    "\n",
    "- Copie a URL do navegador (ex: `https://dbc-a1b2c3d4-e5f6.cloud.databricks.com`)\n",
    "\n",
    "\n",
    "Voc√™ pode copiar a partir do browser s√≥ ate `...com`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">üîë 2. Personal Access Token</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. No Databricks workspace ‚Üí clique no seu **avatar** (canto superior direito)\n",
    "\n",
    "2. **User Settings**\n",
    "\n",
    "3. **Developer** (no menu lateral)\n",
    "\n",
    "4. **Access Tokens**\n",
    "\n",
    "5. **Generate New Token**\n",
    "\n",
    "6. D√™ um nome (ex: \"Local Development\")\n",
    "\n",
    "7. Defina expira√ß√£o (recomendo 90 dias)\n",
    "\n",
    "8. **Generate** ‚Üí copie o token (come√ßa com `dapi-...`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">üíª 3. Cluster ID</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. No Databricks workspace ‚Üí **Compute** (menu lateral)\n",
    "\n",
    "2. Clique no cluster que deseja usar\n",
    "\n",
    "3. Copie o **Cluster ID** da URL ou das configura√ß√µes do cluster\n",
    "\n",
    "\n",
    "`NOTA`:\n",
    "\n",
    "Eu s√≥ consegui obter o ID do meu Cluster executando o seguinte comando na pr√≥pria c√©lula do Databricks:\n",
    "\n",
    "```python\n",
    "databricks_cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "\n",
    "print(f\"Databricks Cluster ID: {databricks_cluster_id}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">ETL Pipeline com dados armazenados no meu Volume Databricks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Carregado minhas credenciais do Databricks, com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "databricks_host = os.environ['DATABRICKS_HOST']\n",
    "databricks_token = os.environ['DATABRICKS_TOKEN']\n",
    "databricks_cluster_id = os.environ['DATABRICKS_CLUSTER_ID']\n",
    "\n",
    "print(\"üîó Carregado minhas credenciais do Databricks, com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir vamos usar `Databricks Connect` para conectar ao Databricks e executar c√≥digo `Spark` localmente, mas processando os dados remotamente no cluster Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado ao Databricks com sucesso!\n",
      "üîß Vers√£o Spark: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "from databricks.connect import DatabricksSession # Esta classe √© usada para conectar ao Databricks remotamente\n",
    "\n",
    "spark = DatabricksSession.builder.remote(host=databricks_host, token=databricks_token, cluster_id=databricks_cluster_id).getOrCreate()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Conectado ao Databricks com sucesso!\")\n",
    "print(f\"üîß Vers√£o Spark: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir vamos carregar dados do `Volume Databricks`. Basicamente, vou carregar esses dados a partir do `Catalog` do meu workspace Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Carregando dados do Volume: /Volumes/workspace/default_eddy/volumeeddy-tmp-sampledata/sample_data.csv\n"
     ]
    }
   ],
   "source": [
    "volume_path = \"/Volumes/workspace/default_eddy/volumeeddy-tmp-sampledata/sample_data.csv\"\n",
    "print(f\"\\nüìÇ Carregando dados do Volume: {volume_path}\")\n",
    "\n",
    "df_spark = spark.read.csv(volume_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ DADOS DO VOLUME CARREGADOS COM SUCESSO!\n",
      "\n",
      "üìä Primeiros registros:\n",
      "+-------+---+-----------+\n",
      "|   Name|Age|       City|\n",
      "+-------+---+-----------+\n",
      "|  Alice| 25|   New York|\n",
      "|    Bob| 17|Los Angeles|\n",
      "|Charlie| 35|    Chicago|\n",
      "|  Diana| 16|    Houston|\n",
      "| Edward| 45|    Phoenix|\n",
      "+-------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üé≤ DADOS DO VOLUME CARREGADOS COM SUCESSO!\")\n",
    "print(\"\\nüìä Primeiros registros:\")\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Informa√ß√µes do Dataset que tenho no meu Volume Databricks:\n",
      "\n",
      "   ‚Ä¢ Quantidade de linhas: 5\n",
      "   ‚Ä¢ Quantidade de colunas: 3\n",
      "   ‚Ä¢ Nomes das colunas: ['Name', 'Age', 'City']\n"
     ]
    }
   ],
   "source": [
    "print(f\"üìà Informa√ß√µes do Dataset que tenho no meu Volume Databricks:\\n\")\n",
    "print(f\"   ‚Ä¢ Quantidade de linhas: {df_spark.count()}\")\n",
    "print(f\"   ‚Ä¢ Quantidade de colunas: {len(df_spark.columns)}\")\n",
    "print(f\"   ‚Ä¢ Nomes das colunas: {df_spark.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Schema do Dataset:\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìã Schema do Dataset:\\n\")\n",
    "\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">A seguir vamos converter para `Pandas` para an√°lises adicionais (`opcional`).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dados tamb√©m dispon√≠veis como Pandas DataFrame para an√°lises locais!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>17</td>\n",
       "      <td>Los Angeles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>35</td>\n",
       "      <td>Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diana</td>\n",
       "      <td>16</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edward</td>\n",
       "      <td>45</td>\n",
       "      <td>Phoenix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name  Age         City\n",
       "0    Alice   25     New York\n",
       "1      Bob   17  Los Angeles\n",
       "2  Charlie   35      Chicago\n",
       "3    Diana   16      Houston\n",
       "4   Edward   45      Phoenix"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"‚úÖ Dados tamb√©m dispon√≠veis como Pandas DataFrame para an√°lises locais!\\n\")\n",
    "\n",
    "df_pandas = df_spark.toPandas()\n",
    "\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"gree\">üìä An√°lise avan√ßada dos dados do Volume Databricks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">Estat√≠sticas descritivas com Spark</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+-------+\n",
      "|summary|  Name|               Age|   City|\n",
      "+-------+------+------------------+-------+\n",
      "|  count|     5|                 5|      5|\n",
      "|   mean|  NULL|              27.6|   NULL|\n",
      "| stddev|  NULL|12.361229712289955|   NULL|\n",
      "|    min| Alice|                16|Chicago|\n",
      "|    max|Edward|                45|Phoenix|\n",
      "+-------+------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"yellow\">An√°lises com Spark SQL</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar como view tempor√°ria para usar SQL:\n",
    "df_spark.createOrReplaceTempView(\"pessoas\") # Nome da view tempor√°ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----------+\n",
      "|   Name|Age|       City|\n",
      "+-------+---+-----------+\n",
      "|  Alice| 25|   New York|\n",
      "|    Bob| 17|Los Angeles|\n",
      "|Charlie| 35|    Chicago|\n",
      "|  Diana| 16|    Houston|\n",
      "| Edward| 45|    Phoenix|\n",
      "+-------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A minha view tempor√°ria:\n",
    "spark.sql(\"SELECT * FROM pessoas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS:\n",
    "\n",
    "```sql\n",
    "spark.sql(\"SELECT * FROM pessoas\")           # Nome da tabela criada na view tempor√°ria\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë• An√°lise de Idade:\n",
      "\n",
      "+-----------+------------+------------+-------------+\n",
      "|idade_media|idade_minima|idade_maxima|total_pessoas|\n",
      "+-----------+------------+------------+-------------+\n",
      "|       27.6|          16|          45|            5|\n",
      "+-----------+------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. An√°lise de idade\n",
    "print(\"üë• An√°lise de Idade:\\n\")\n",
    "\n",
    "idade_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        AVG(Age) as idade_media,\n",
    "        MIN(Age) as idade_minima,\n",
    "        MAX(Age) as idade_maxima,\n",
    "        COUNT(*) as total_pessoas\n",
    "    FROM pessoas\n",
    "\"\"\")\n",
    "\n",
    "idade_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîû Pessoas maiores de idade:\n",
      "\n",
      "+-------+---+--------+\n",
      "|   Name|Age|    City|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|New York|\n",
      "|Charlie| 35| Chicago|\n",
      "| Edward| 45| Phoenix|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Maiores de idade\n",
    "print(\"üîû Pessoas maiores de idade:\\n\")\n",
    "\n",
    "adults = spark.sql(\"SELECT * FROM pessoas WHERE Age >= 18\")\n",
    "adults.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 de 5 pessoas s√£o maiores de idade (60.0%)\n"
     ]
    }
   ],
   "source": [
    "adult_count = adults.count()\n",
    "total_count = df_spark.count()\n",
    "\n",
    "print(f\"{adult_count} de {total_count} pessoas s√£o maiores de idade ({adult_count/total_count*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. An√°lise por cidade\n",
    "print(\"üèôÔ∏è Contagem por Cidade:\\n\")\n",
    "\n",
    "city_analysis = spark.sql(\"\"\"\n",
    "    SELECT City, COUNT(*) as quantidade\n",
    "    FROM pessoas \n",
    "    GROUP BY City \n",
    "    ORDER BY quantidade DESC\n",
    "\"\"\")\n",
    "city_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4. An√°lises avan√ßadas com Window Functions\n",
    "print(\"\\nüìà An√°lises Avan√ßadas:\")\n",
    "advanced_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Name,\n",
    "        Age,\n",
    "        City,\n",
    "        CASE \n",
    "            WHEN Age >= 18 THEN 'Adulto'\n",
    "            ELSE 'Menor de idade'\n",
    "        END as categoria_idade,\n",
    "        ROW_NUMBER() OVER (PARTITION BY City ORDER BY Age DESC) as ranking_idade_cidade\n",
    "    FROM pessoas\n",
    "    ORDER BY City, Age DESC\n",
    "\"\"\")\n",
    "advanced_analysis.show()\n",
    "\n",
    "# Converter para Pandas para an√°lises complementares\n",
    "print(\"\\nüêº An√°lises complementares com Pandas:\")\n",
    "df_pandas = df_spark.toPandas()\n",
    "print(f\"   ‚Ä¢ Dataset convertido: {df_pandas.shape[0]} linhas, {df_pandas.shape[1]} colunas\")\n",
    "print(f\"   ‚Ä¢ Idade m√©dia: {df_pandas['Age'].mean():.2f} anos\")\n",
    "print(f\"   ‚Ä¢ Cidades √∫nicas: {df_pandas['City'].nunique()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline ETL com Databricks Volume conclu√≠do com sucesso!\")\n",
    "print(\"   ‚Ä¢ üå©Ô∏è Processamento: Cluster Databricks\")\n",
    "print(\"   ‚Ä¢ üìÅ Fonte: Volume Databricks\")\n",
    "print(f\"   ‚Ä¢ üìä Registros: {df_spark.count()}\")\n",
    "print(f\"   ‚Ä¢ üöÄ Engine: Spark {spark.version}\")\n",
    "print(\"   ‚Ä¢ üíæ Dispon√≠vel em: Spark DataFrame + Pandas DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline_ETL",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
