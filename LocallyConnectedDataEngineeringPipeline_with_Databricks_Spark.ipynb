{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Data Pipeline in Databricks and using Spark\n",
    "---\n",
    "\n",
    "#### Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Configuração do Databricks Connect\n",
    "\n",
    "Este notebook está configurado para usar **Databricks Connect**, permitindo executar código localmente mas processando dados no cluster Databricks.\n",
    "\n",
    "## 🔐 Informações necessárias:\n",
    "\n",
    "1. **Server hostname**: URL do seu workspace Databricks\n",
    "2. **Personal Access Token**: Token de acesso (User Settings → Developer → Access Tokens)\n",
    "3. **Cluster ID**: ID do cluster ativo (copie da URL quando abrir um cluster)\n",
    "\n",
    "## 📋 Como configurar:\n",
    "\n",
    "### 1️⃣ Criar variáveis de ambiente (recomendado):\n",
    "```bash\n",
    "export DATABRICKS_HOST=\"https://seu-workspace.cloud.databricks.com\"\n",
    "export DATABRICKS_TOKEN=\"dapi-seu-token-aqui\"  \n",
    "export DATABRICKS_CLUSTER_ID=\"cluster-id-aqui\"\n",
    "```\n",
    "\n",
    "### 2️⃣ Ou configure diretamente no código abaixo ⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📋 COMO OBTER AS CREDENCIAIS DO DATABRICKS\n",
    "\n",
    "## 🔍 1. Server Hostname\n",
    "- Va para seu workspace Databricks\n",
    "- Copie a URL do navegador (ex: `https://dbc-a1b2c3d4-e5f6.cloud.databricks.com`)\n",
    "\n",
    "## 🔑 2. Personal Access Token\n",
    "1. No Databricks workspace → clique no seu **avatar** (canto superior direito)\n",
    "2. **User Settings**\n",
    "3. **Developer** (no menu lateral)\n",
    "4. **Access Tokens**\n",
    "5. **Generate New Token**\n",
    "6. Dê um nome (ex: \"Local Development\")\n",
    "7. Defina expiração (recomendo 90 dias)\n",
    "8. **Generate** → copie o token (começa com `dapi-...`)\n",
    "\n",
    "## 💻 3. Cluster ID\n",
    "1. No Databricks workspace → **Compute** (menu lateral)\n",
    "2. Clique no cluster que deseja usar\n",
    "3. Copie o **Cluster ID** da URL ou das configurações do cluster\n",
    "\n",
    "## 🔒 4. Configurar de forma segura\n",
    "### Opção A: Variáveis de ambiente (recomendado)\n",
    "```bash\n",
    "export DATABRICKS_HOST=\"https://seu-workspace.cloud.databricks.com\"\n",
    "export DATABRICKS_TOKEN=\"dapi-seu-token-aqui\"\n",
    "export DATABRICKS_CLUSTER_ID=\"cluster-id-aqui\"\n",
    "```\n",
    "\n",
    "### Opção B: Arquivo .env (alternativa)\n",
    "Crie um arquivo `.env` na pasta do projeto:\n",
    "```\n",
    "DATABRICKS_HOST=https://seu-workspace.cloud.databricks.com\n",
    "DATABRICKS_TOKEN=dapi-seu-token-aqui\n",
    "DATABRICKS_CLUSTER_ID=cluster-id-aqui\n",
    "```\n",
    "\n",
    "**⚠️ IMPORTANTE**: Nunca faça commit de tokens/credenciais para Git!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Conectando ao Databricks...\n",
      "📍 Host: https://dbc-15e1a878-daba.cloud.databricks.com\n",
      "🆔 Cluster: 0919-175003-lz63w6ap-v2n\n",
      "✅ Conectado ao Databricks com sucesso!\n",
      "🔧 Versão Spark: 4.0.0\n",
      "\n",
      "📂 Carregando dados do Volume: /Volumes/workspace/default_eddy/volumeeddy-tmp-sampledata/sample_data.csv\n",
      "🎉 DADOS DO VOLUME CARREGADOS COM SUCESSO!\n",
      "\n",
      "📊 Primeiros registros:\n",
      "+-------+---+-----------+\n",
      "|   Name|Age|       City|\n",
      "+-------+---+-----------+\n",
      "|  Alice| 25|   New York|\n",
      "|    Bob| 17|Los Angeles|\n",
      "|Charlie| 35|    Chicago|\n",
      "|  Diana| 16|    Houston|\n",
      "| Edward| 45|    Phoenix|\n",
      "+-------+---+-----------+\n",
      "\n",
      "\n",
      "📈 Informações do Dataset:\n",
      "   • Linhas: 5\n",
      "   • Colunas: 3\n",
      "   • Nomes das Colunas: ['Name', 'Age', 'City']\n",
      "\n",
      "📋 Schema do Dataset:\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n",
      "\n",
      "✅ Dados também disponíveis como Pandas DataFrame para análises locais!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Carregar credenciais\n",
    "_ = load_dotenv(find_dotenv())\n",
    "databricks_host = os.environ['DATABRICKS_HOST']\n",
    "databricks_token = os.environ['DATABRICKS_TOKEN']\n",
    "databricks_cluster_id = os.environ['DATABRICKS_CLUSTER_ID']\n",
    "\n",
    "print(\"🔗 Conectando ao Databricks...\")\n",
    "print(f\"📍 Host: {databricks_host}\")\n",
    "print(f\"🆔 Cluster: {databricks_cluster_id}\")\n",
    "\n",
    "# Conectar ao Databricks\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "spark = DatabricksSession.builder \\\n",
    "    .remote(\n",
    "        host=databricks_host,\n",
    "        token=databricks_token,\n",
    "        cluster_id=databricks_cluster_id\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Conectado ao Databricks com sucesso!\")\n",
    "print(f\"🔧 Versão Spark: {spark.version}\")\n",
    "\n",
    "# 📁 CARREGAR DADOS DO VOLUME DATABRICKS\n",
    "volume_path = \"/Volumes/workspace/default_eddy/volumeeddy-tmp-sampledata/sample_data.csv\"\n",
    "print(f\"\\n📂 Carregando dados do Volume: {volume_path}\")\n",
    "\n",
    "df_spark = spark.read.csv(volume_path, header=True, inferSchema=True)\n",
    "\n",
    "print(\"🎉 DADOS DO VOLUME CARREGADOS COM SUCESSO!\")\n",
    "print(\"\\n📊 Primeiros registros:\")\n",
    "df_spark.show()\n",
    "\n",
    "print(f\"\\n📈 Informações do Dataset:\")\n",
    "print(f\"   • Linhas: {df_spark.count()}\")\n",
    "print(f\"   • Colunas: {len(df_spark.columns)}\")\n",
    "print(f\"   • Nomes das Colunas: {df_spark.columns}\")\n",
    "\n",
    "print(\"\\n📋 Schema do Dataset:\")\n",
    "df_spark.printSchema()\n",
    "\n",
    "# Converter para Pandas para análises adicionais (opcional)\n",
    "df_pandas = df_spark.toPandas()\n",
    "print(f\"\\n✅ Dados também disponíveis como Pandas DataFrame para análises locais!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "📊 ANÁLISE COMPLETA DOS DADOS DO VOLUME DATABRICKS\n",
      "======================================================================\n",
      "\n",
      "📊 Estatísticas Descritivas (processadas no cluster Databricks):\n",
      "+-------+------+------------------+-------+\n",
      "|summary|  Name|               Age|   City|\n",
      "+-------+------+------------------+-------+\n",
      "|  count|     5|                 5|      5|\n",
      "|   mean|  NULL|              27.6|   NULL|\n",
      "| stddev|  NULL|12.361229712289955|   NULL|\n",
      "|    min| Alice|                16|Chicago|\n",
      "|    max|Edward|                45|Phoenix|\n",
      "+-------+------+------------------+-------+\n",
      "\n",
      "\n",
      "🔍 Análises com Spark SQL:\n",
      "\n",
      "👥 Análise de Idade:\n",
      "+-----------+------------+------------+-------------+\n",
      "|idade_media|idade_minima|idade_maxima|total_pessoas|\n",
      "+-----------+------------+------------+-------------+\n",
      "|       27.6|          16|          45|            5|\n",
      "+-----------+------------+------------+-------------+\n",
      "\n",
      "\n",
      "🔞 Pessoas maiores de idade:\n",
      "+-------+---+--------+\n",
      "|   Name|Age|    City|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|New York|\n",
      "|Charlie| 35| Chicago|\n",
      "| Edward| 45| Phoenix|\n",
      "+-------+---+--------+\n",
      "\n",
      "📈 3 de 5 pessoas são maiores de idade (60.0%)\n",
      "\n",
      "🏙️ Contagem por Cidade:\n",
      "+-----------+----------+\n",
      "|       City|quantidade|\n",
      "+-----------+----------+\n",
      "|Los Angeles|         1|\n",
      "|   New York|         1|\n",
      "|    Houston|         1|\n",
      "|    Phoenix|         1|\n",
      "|    Chicago|         1|\n",
      "+-----------+----------+\n",
      "\n",
      "\n",
      "📈 Análises Avançadas:\n",
      "+-------+---+-----------+---------------+--------------------+\n",
      "|   Name|Age|       City|categoria_idade|ranking_idade_cidade|\n",
      "+-------+---+-----------+---------------+--------------------+\n",
      "|Charlie| 35|    Chicago|         Adulto|                   1|\n",
      "|  Diana| 16|    Houston| Menor de idade|                   1|\n",
      "|    Bob| 17|Los Angeles| Menor de idade|                   1|\n",
      "|  Alice| 25|   New York|         Adulto|                   1|\n",
      "| Edward| 45|    Phoenix|         Adulto|                   1|\n",
      "+-------+---+-----------+---------------+--------------------+\n",
      "\n",
      "\n",
      "🐼 Análises complementares com Pandas:\n",
      "   • Dataset convertido: 5 linhas, 3 colunas\n",
      "   • Idade média: 27.60 anos\n",
      "   • Cidades únicas: 5\n",
      "\n",
      "✅ Pipeline ETL com Databricks Volume concluído com sucesso!\n",
      "   • 🌩️ Processamento: Cluster Databricks\n",
      "   • 📁 Fonte: Volume Databricks\n",
      "   • 📊 Registros: 5\n",
      "   • 🚀 Engine: Spark 4.0.0\n",
      "   • 💾 Disponível em: Spark DataFrame + Pandas DataFrame\n"
     ]
    }
   ],
   "source": [
    "# 📊 ANÁLISE AVANÇADA DOS DADOS DO VOLUME DATABRICKS\n",
    "print(\"=\"*70)\n",
    "print(\"📊 ANÁLISE COMPLETA DOS DADOS DO VOLUME DATABRICKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Estatísticas descritivas com Spark\n",
    "print(\"\\n📊 Estatísticas Descritivas (processadas no cluster Databricks):\")\n",
    "df_spark.describe().show()\n",
    "\n",
    "# Análises com Spark SQL\n",
    "print(\"\\n🔍 Análises com Spark SQL:\")\n",
    "\n",
    "# Registrar como view temporária para usar SQL\n",
    "df_spark.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "# 1. Análise de idade\n",
    "print(\"\\n👥 Análise de Idade:\")\n",
    "idade_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        AVG(Age) as idade_media,\n",
    "        MIN(Age) as idade_minima,\n",
    "        MAX(Age) as idade_maxima,\n",
    "        COUNT(*) as total_pessoas\n",
    "    FROM pessoas\n",
    "\"\"\")\n",
    "idade_stats.show()\n",
    "\n",
    "# 2. Maiores de idade\n",
    "print(\"\\n🔞 Pessoas maiores de idade:\")\n",
    "adults = spark.sql(\"SELECT * FROM pessoas WHERE Age >= 18\")\n",
    "adults.show()\n",
    "\n",
    "adult_count = adults.count()\n",
    "total_count = df_spark.count()\n",
    "print(f\"📈 {adult_count} de {total_count} pessoas são maiores de idade ({adult_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# 3. Análise por cidade\n",
    "print(\"\\n🏙️ Contagem por Cidade:\")\n",
    "city_analysis = spark.sql(\"\"\"\n",
    "    SELECT City, COUNT(*) as quantidade\n",
    "    FROM pessoas \n",
    "    GROUP BY City \n",
    "    ORDER BY quantidade DESC\n",
    "\"\"\")\n",
    "city_analysis.show()\n",
    "\n",
    "# 4. Análises avançadas com Window Functions\n",
    "print(\"\\n📈 Análises Avançadas:\")\n",
    "advanced_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Name,\n",
    "        Age,\n",
    "        City,\n",
    "        CASE \n",
    "            WHEN Age >= 18 THEN 'Adulto'\n",
    "            ELSE 'Menor de idade'\n",
    "        END as categoria_idade,\n",
    "        ROW_NUMBER() OVER (PARTITION BY City ORDER BY Age DESC) as ranking_idade_cidade\n",
    "    FROM pessoas\n",
    "    ORDER BY City, Age DESC\n",
    "\"\"\")\n",
    "advanced_analysis.show()\n",
    "\n",
    "# Converter para Pandas para análises complementares\n",
    "print(\"\\n🐼 Análises complementares com Pandas:\")\n",
    "df_pandas = df_spark.toPandas()\n",
    "print(f\"   • Dataset convertido: {df_pandas.shape[0]} linhas, {df_pandas.shape[1]} colunas\")\n",
    "print(f\"   • Idade média: {df_pandas['Age'].mean():.2f} anos\")\n",
    "print(f\"   • Cidades únicas: {df_pandas['City'].nunique()}\")\n",
    "\n",
    "print(\"\\n✅ Pipeline ETL com Databricks Volume concluído com sucesso!\")\n",
    "print(\"   • 🌩️ Processamento: Cluster Databricks\")\n",
    "print(\"   • 📁 Fonte: Volume Databricks\")\n",
    "print(f\"   • 📊 Registros: {df_spark.count()}\")\n",
    "print(f\"   • 🚀 Engine: Spark {spark.version}\")\n",
    "print(\"   • 💾 Disponível em: Spark DataFrame + Pandas DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1728710-0082-4be5-a91d-b8cb48508b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline_ETL",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
