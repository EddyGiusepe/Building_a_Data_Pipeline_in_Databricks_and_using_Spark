import os
from dotenv import load_dotenv, find_dotenv

# Carregar credenciais
_ = load_dotenv(find_dotenv())
databricks_host = os.environ['DATABRICKS_HOST']
databricks_token = os.environ['DATABRICKS_TOKEN']
databricks_cluster_id = os.environ['DATABRICKS_CLUSTER_ID']

print("ğŸ”— Conectando ao Databricks...")
print(f"ğŸ“ Host: {databricks_host}")
print(f"ğŸ†” Cluster: {databricks_cluster_id}")

# Conectar ao Databricks
from databricks.connect import DatabricksSession

spark = DatabricksSession.builder \
    .remote(
        host=databricks_host,
        token=databricks_token,
        cluster_id=databricks_cluster_id
    ) \
    .getOrCreate()

print("âœ… Conectado ao Databricks com sucesso!")
print(f"ğŸ”§ VersÃ£o Spark: {spark.version}")

# ğŸ“ CARREGAR DADOS DO VOLUME DATABRICKS
volume_path = "/Volumes/workspace/default_eddy/volumeeddy-tmp-sampledata/sample_data.csv"
print(f"\nğŸ“‚ Carregando dados do Volume: {volume_path}")

df_spark = spark.read.csv(volume_path, header=True, inferSchema=True)

print("ğŸ‰ DADOS DO VOLUME CARREGADOS COM SUCESSO!")
print("\nğŸ“Š Primeiros registros:")
df_spark.show()

print(f"\nğŸ“ˆ InformaÃ§Ãµes do Dataset:")
print(f"   â€¢ Linhas: {df_spark.count()}")
print(f"   â€¢ Colunas: {len(df_spark.columns)}")
print(f"   â€¢ Nomes das Colunas: {df_spark.columns}")

print("\nğŸ“‹ Schema do Dataset:")
df_spark.printSchema()

print("\nğŸ“Š EstatÃ­sticas Descritivas:")
df_spark.describe().show()

# AnÃ¡lises com Spark SQL
print("\nğŸ” AnÃ¡lises com Spark SQL:")
df_spark.createOrReplaceTempView("pessoas")

# Maiores de idade
print("\nğŸ” Pessoas maiores de idade:")
adults = spark.sql("SELECT * FROM pessoas WHERE Age >= 18")
adults.show()

# AnÃ¡lise por cidade
print("\nğŸ™ï¸ Contagem por Cidade:")
city_analysis = spark.sql("""
    SELECT City, COUNT(*) as quantidade
    FROM pessoas 
    GROUP BY City 
    ORDER BY quantidade DESC
""")
city_analysis.show()

# Converter para Pandas para anÃ¡lises adicionais (opcional)
df_pandas = df_spark.toPandas()
print(f"\nğŸ¼ Dados tambÃ©m disponÃ­veis como Pandas: {df_pandas.shape[0]} linhas, {df_pandas.shape[1]} colunas")

print("\nâœ… Pipeline ETL com Volume Databricks concluÃ­do com sucesso!")
print("   â€¢ ğŸŒ©ï¸ Processamento: Cluster Databricks")
print("   â€¢ ğŸ“ Fonte: Volume Databricks")
print(f"   â€¢ ğŸ“Š Registros: {df_spark.count()}")
print(f"   â€¢ ğŸš€ Engine: Spark {spark.version}")
