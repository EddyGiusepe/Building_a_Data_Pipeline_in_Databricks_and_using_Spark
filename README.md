# <h1 align="center"><font color="gree">Building a Data Pipeline in Databricks and using Spark</font></h1>
---

<font color="pink">Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>


A Databricks oferece uma plataforma integrada para processamento de grandes volumes de dados de forma escalÃ¡vel. Neste guia, vamos construir um pipeline de dados utilizando Apache Spark, explorando desde a ingestÃ£o atÃ© a transformaÃ§Ã£o e armazenamento, com foco na automaÃ§Ã£o para projetos de data Engineering e Analytics.


![](./img/ETL_Pipeline_Databricks.jpeg)


Neste projeto de Engenharia de Dados basicamente vamos a Extrair, Transformar e Carregar os dados transformados. Para isso vamos usar Spark e construÃ­r um pipeline de dados. 

![](./img/Extract_Transform_Load.jpeg)


## Por que usar Databricks para Pipelines?

* Processa dados em paralelo
* Suporta mÃºltiplas linguagens como `Python`, `SQL` e `Scala`.
* Interface intuitiva de notebook para codificaÃ§Ã£o e colaboraÃ§Ã£o.



## ğŸ”§ ConexÃ£o com o Databricks para trabalhar diretamente com os dados do Volume

### 1ï¸âƒ£ **Corrigida Incompatibilidade de VersÃµes**
- âŒ **Antes**: `databricks-connect 17.2.0` vs `Runtime 17.1` 
- âœ… **Agora**: `databricks-connect 15.4.14` compatÃ­vel com `Runtime 17.1`

### 2ï¸âƒ£ **ConexÃ£o Estabelecida**
- âœ… Conectado ao cluster: `0919-175003-lz63w6ap-v2n`
- âœ… Acessando Volume: `/Volumes/workspace/default_eddy/volumeeddy-tmp-sampledata/`
- âœ… Spark versÃ£o: `4.0.0` funcionando

### 3ï¸âƒ£ **Pipeline ETL Completo**
- âœ… Carregamento de dados do Volume Databricks
- âœ… AnÃ¡lises com Spark SQL
- âœ… Window Functions avanÃ§adas
- âœ… ConversÃ£o para Pandas quando necessÃ¡rio

## ğŸ“Š **Funcionalidades Implementadas**

### **ğŸ”— ConexÃ£o Databricks**
```python
spark = DatabricksSession.builder.remote(
    host=databricks_host,
    token=databricks_token,
    cluster_id=databricks_cluster_id
).getOrCreate()
```

### **ğŸ“ Carregamento do Volume**
```python
volume_path = "/Volumes/workspace/default_eddy/volumeeddy-tmp-sampledata/sample_data.csv"
df_spark = spark.read.csv(volume_path, header=True, inferSchema=True)
```

### **ğŸ” AnÃ¡lises AvanÃ§adas**
- **EstatÃ­sticas descritivas** processadas no cluster
- **Spark SQL** para consultas complexas  
- **Window Functions** para rankings e anÃ¡lises
- **CategorizaÃ§Ã£o** de dados (adultos vs menores)
- **AgregaÃ§Ãµes** por cidade
- **ConversÃ£o para Pandas** para anÃ¡lises locais

## ğŸš€ **Como usar agora:**

### **1. Execute as cÃ©lulas do notebook na ordem:**
- Carrega credenciais
- Conecta e carrega dados do Volume
- AnÃ¡lises avanÃ§adas com Spark SQL

### **2. Seus dados estÃ£o disponÃ­veis como:**
- `df_spark`: DataFrame Spark (processamento no cluster)
- `df_pandas`: DataFrame Pandas (anÃ¡lises locais)

### **3. VocÃª pode fazer anÃ¡lises como:**
```sql
-- Exemplo de consulta SQL no Spark
spark.sql("""
    SELECT City, AVG(Age) as idade_media
    FROM pessoas 
    GROUP BY City
    ORDER BY idade_media DESC
""").show()
```




Thank God!