"""
Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro

Script databricks_connection_test.py
====================================
Este script testa a conexÃ£o com o Databricks e carrega dados do Volume Databricks.

RUN
---
uv run databricks_connection_test.py
"""
import os
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv())

# Carregar credenciais obtidas no Databricks:
databricks_host = os.environ['DATABRICKS_HOST']
databricks_token = os.environ['DATABRICKS_TOKEN']
databricks_cluster_id = os.environ['DATABRICKS_CLUSTER_ID']

print("ğŸ”— Testando conexÃ£o com Databricks...")
print(f"ğŸ“ Host: {databricks_host}")
print(f"ğŸ†” Cluster: {databricks_cluster_id}")

try:
    # Tentar conexÃ£o Databricks
    from databricks.connect import DatabricksSession
    
    spark = DatabricksSession.builder \
        .remote(
            host=databricks_host,
            token=databricks_token,
            cluster_id=databricks_cluster_id
        ) \
        .getOrCreate()
    
    print("âœ… Conectado ao Databricks com sucesso!")
    
    # Teste bÃ¡sico de conexÃ£o
    test_df = spark.sql("SELECT 1 as test")
    test_df.show()
    
    # Carregar dados do Volume Databricks
    volume_path = "/Volumes/workspace/default_eddy/volumeeddy-tmp-sampledata/sample_data.csv"
    print(f"ğŸ“‚ Carregando dados de: {volume_path}")
    
    df = spark.read.csv(volume_path, header=True, inferSchema=True)
    print("ğŸ“Š Dados carregados do Databricks:")
    df.show(5)
    print(f"ğŸ“ˆ Shape: {df.count()} linhas, {len(df.columns)} colunas")
    
except Exception as e:
    print(f"âŒ Erro Databricks: {e}")
    print("\nğŸ”„ Usando PySpark local puro...")
    
    # Importar PySpark padrÃ£o (nÃ£o Connect)
    from pyspark.sql import SparkSession
    
    # Criar sessÃ£o Spark local PURA
    spark = SparkSession.builder \
        .appName("LocalSparkTest") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "false") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "false") \
        .getOrCreate()
    
    print("âœ… Spark local inicializado!")
    
    # Testar com arquivo local
    csv_path = os.path.join(os.getcwd(), "sample_data.csv")
    print(f"ğŸ“‚ Carregando dados de: {csv_path}")
    
    df = spark.read.csv(csv_path, header=True, inferSchema=True)
    print("ğŸ“Š Dados carregados localmente:")
    df.show(5)
    print(f"ğŸ“ˆ Shape: {df.count()} linhas, {len(df.columns)} colunas")
    
    # AnÃ¡lise bÃ¡sica
    print("\nğŸ“Š AnÃ¡lise bÃ¡sica:")
    df.describe().show()

print("\nâœ… Teste concluÃ­do!")
